{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Prefecto Prefecto is a collection of tools to extend and augment Prefect capabilities. Getting Started Install Prefecto with pip install prefecto Prefecto is only tested with Python 3.10 and higher. It may work with older versions, but it is not guaranteed. Break up Task.map into batches . from prefect import flow, task from prefecto.concurrency import BatchTask @task def task(x): print(x) return x @flow def flow(): results = BatchTask(task, size=100).map(range(1000)) Standard serializers for pandas.DataFrame and polars.DataFrame . import polars as pl from prefect import flow, task from prefecto.serialization.polars import PolarsSerializer @task(serializer=PolarsSerializer(method=\"polars.parquet\"), persist_result=True, cache_result_in_memory=False) def parquet_task(df: pl.DataFrame) -> pl.DataFrame: ... @task(serializer=PolarsSerializer(method=\"polars.csv\"), persist_result=True, cache_result_in_memory=False) def csv_task(df: pl.DataFrame) -> pl.DataFrame: ... @flow def flow(): df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) df = parquet_task(df) df = csv_task(df) return df Extras Prefecto includes a number of extras that are not installed by default. Extra Description pandas Adds support for pandas.DataFrame serialization. polars Adds support for polars.DataFrame serialization. moto Adds support for mocking AWS's boto3 with moto Extras can be installed with pip install prefecto[extra]","title":"Home"},{"location":"#prefecto","text":"Prefecto is a collection of tools to extend and augment Prefect capabilities.","title":"Prefecto"},{"location":"#getting-started","text":"Install Prefecto with pip install prefecto Prefecto is only tested with Python 3.10 and higher. It may work with older versions, but it is not guaranteed. Break up Task.map into batches . from prefect import flow, task from prefecto.concurrency import BatchTask @task def task(x): print(x) return x @flow def flow(): results = BatchTask(task, size=100).map(range(1000)) Standard serializers for pandas.DataFrame and polars.DataFrame . import polars as pl from prefect import flow, task from prefecto.serialization.polars import PolarsSerializer @task(serializer=PolarsSerializer(method=\"polars.parquet\"), persist_result=True, cache_result_in_memory=False) def parquet_task(df: pl.DataFrame) -> pl.DataFrame: ... @task(serializer=PolarsSerializer(method=\"polars.csv\"), persist_result=True, cache_result_in_memory=False) def csv_task(df: pl.DataFrame) -> pl.DataFrame: ... @flow def flow(): df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) df = parquet_task(df) df = csv_task(df) return df","title":"Getting Started"},{"location":"#extras","text":"Prefecto includes a number of extras that are not installed by default. Extra Description pandas Adds support for pandas.DataFrame serialization. polars Adds support for polars.DataFrame serialization. moto Adds support for mocking AWS's boto3 with moto Extras can be installed with pip install prefecto[extra]","title":"Extras"},{"location":"concurrency/","text":"Concurrency Module for advanced concurrency control. BatchTask Wraps a Task to perform Task.map in batches. Parameters: task ( Task ) \u2013 The task to wrap. size ( int ) \u2013 The size of the batches to perform Task.map on. map ( * args , ** kwds ) Perform a Task.map operation in batches of the keyword arguments. The arguments must be iterables of equal length. Parameters: *args \u2013 Positional arguments to pass to the task. **kwds \u2013 Keyword arguments to pass to the task. Returns: list [ PrefectFuture ] \u2013 A list of futures for each batch. Examples: from prefect import flow, task from prefecto.concurrency import BatchTask @task def add(a, b): return a + b @flow def my_flow(): batch_add = BatchTask(add, 2) return batch_add.map([1,2,3,4], [2,3,4,5]) print(my_flow()) $ python my_flow.py 01:31:51.012 | INFO | prefect.engine - Created flow run 'beryl-moth' for flow 'test' 01:31:52.238 | DEBUG | Flow run 'beryl-moth' - Mapping 'add' batch 1 of 2. 01:31:52.239 | INFO | Flow run 'beryl-moth' - Created task run 'add-0' for task 'add' 01:31:52.240 | INFO | Flow run 'beryl-moth' - Submitted task run 'add-0' for execution. 01:31:52.253 | INFO | Flow run 'beryl-moth' - Created task run 'add-1' for task 'add' 01:31:52.254 | INFO | Flow run 'beryl-moth' - Submitted task run 'add-1' for execution. 01:31:52.259 | DEBUG | Flow run 'beryl-moth' - Mapping 'add' batch 2 of 2. 01:31:52.258 | INFO | Flow run 'beryl-moth' - Created task run 'add-3' for task 'add' 01:31:52.258 | INFO | Flow run 'beryl-moth' - Submitted task run 'add-3' for execution. 01:31:52.260 | INFO | Flow run 'beryl-moth' - Created task run 'add-2' for task 'add' 01:31:52.261 | INFO | Flow run 'beryl-moth' - Submitted task run 'add-2' for execution. 01:31:52.675 | INFO | Task run 'add-1' - Finished in state Completed() 01:31:52.770 | INFO | Task run 'add-0' - Finished in state Completed() 01:31:52.885 | INFO | Task run 'add-2' - Finished in state Completed() 01:31:53.075 | INFO | Task run 'add-3' - Finished in state Completed() 01:31:53.979 | INFO | Flow run 'beryl-moth' - Finished in state Completed() [3, 5, 7, 9]","title":"Concurrency"},{"location":"concurrency/#concurrency","text":"Module for advanced concurrency control.","title":"Concurrency"},{"location":"concurrency/#batchtask","text":"Wraps a Task to perform Task.map in batches. Parameters: task ( Task ) \u2013 The task to wrap. size ( int ) \u2013 The size of the batches to perform Task.map on.","title":"BatchTask"},{"location":"concurrency/#src.prefecto.concurrency.BatchTask.map","text":"Perform a Task.map operation in batches of the keyword arguments. The arguments must be iterables of equal length. Parameters: *args \u2013 Positional arguments to pass to the task. **kwds \u2013 Keyword arguments to pass to the task. Returns: list [ PrefectFuture ] \u2013 A list of futures for each batch. Examples: from prefect import flow, task from prefecto.concurrency import BatchTask @task def add(a, b): return a + b @flow def my_flow(): batch_add = BatchTask(add, 2) return batch_add.map([1,2,3,4], [2,3,4,5]) print(my_flow()) $ python my_flow.py 01:31:51.012 | INFO | prefect.engine - Created flow run 'beryl-moth' for flow 'test' 01:31:52.238 | DEBUG | Flow run 'beryl-moth' - Mapping 'add' batch 1 of 2. 01:31:52.239 | INFO | Flow run 'beryl-moth' - Created task run 'add-0' for task 'add' 01:31:52.240 | INFO | Flow run 'beryl-moth' - Submitted task run 'add-0' for execution. 01:31:52.253 | INFO | Flow run 'beryl-moth' - Created task run 'add-1' for task 'add' 01:31:52.254 | INFO | Flow run 'beryl-moth' - Submitted task run 'add-1' for execution. 01:31:52.259 | DEBUG | Flow run 'beryl-moth' - Mapping 'add' batch 2 of 2. 01:31:52.258 | INFO | Flow run 'beryl-moth' - Created task run 'add-3' for task 'add' 01:31:52.258 | INFO | Flow run 'beryl-moth' - Submitted task run 'add-3' for execution. 01:31:52.260 | INFO | Flow run 'beryl-moth' - Created task run 'add-2' for task 'add' 01:31:52.261 | INFO | Flow run 'beryl-moth' - Submitted task run 'add-2' for execution. 01:31:52.675 | INFO | Task run 'add-1' - Finished in state Completed() 01:31:52.770 | INFO | Task run 'add-0' - Finished in state Completed() 01:31:52.885 | INFO | Task run 'add-2' - Finished in state Completed() 01:31:53.075 | INFO | Task run 'add-3' - Finished in state Completed() 01:31:53.979 | INFO | Flow run 'beryl-moth' - Finished in state Completed() [3, 5, 7, 9]","title":"map()"},{"location":"filesystems/","text":"Filesystems Tools to extend Prefect file system behavior. create_child Create a child file system from a parent file system. If path is given, the child file system will have its basepath set to the resolved path. If suffix is given, it will be appended to the child block's _block_document_name . The FileSystem class must have a basepath or bucket_folder attribute and init parameter a _resolve_path method to resolve the path. path . It must also have a _resolve_path method to resolve the path. Parameters: fs ( FileSystem ) \u2013 the parent file system path ( str | Path ) \u2013 the path to the child file system suffix ( str ) \u2013 the suffix to append to the child block's Returns: FileSystem ( FileSystem ) \u2013 the child file system with a resolved basepath Examples: Basic usage: from prefect.filesystems import LocalFileSystem from prefecto.filesystems import create_child fs = LocalFileSystem(basepath=\"base_folder\") child = create_child(fs, path=\"child_folder\") print(child.basepath) base_folder/child_folder With prefect-aws : from prefect_aws import S3Bucket from prefecto.filesystems import create_child fs = S3Bucket(bucket_folder=\"base_folder\") child = create_child(fs, path=\"child_folder\") print(child.bucket_folder) base_folder/child_folder task_persistence_subfolder Decorator to set the task.result_storage attribute to a child file system of fs . This method does not alter the result_serializer or persist_result attributes. Parameters: fs ( FileSystem ) \u2013 the file system to create a child file system from path ( str ) \u2013 the path to the child file system. Defaults to None. suffix ( str ) \u2013 the suffix to append to the child block's _block_document_name . Defaults to None. Returns: Callable \u2013 the decorator to apply to a task Examples: Basic usage: from prefect import task from prefect.filesystems import LocalFileSystem from prefecto.filesystems import task_persistence_subfolder fs = LocalFileSystem(basepath=\"base_folder/\") @task_persistence_subfolder(fs) @task(persist_result=True) def persisted_task(): ... print(persisted_task.result_storage.basepath) base_folder/persisted_task Custom path: from prefect import task from prefect.filesystems import LocalFileSystem from prefecto.filesystems import task_persistence_subfolder fs = LocalFileSystem(basepath=\"base_folder/\") @task_persistence_subfolder(fs, path=\"custom_persisted_task\") @task(persist_result=True) def persisted_task(): ... print(persisted_task.result_storage.basepath) base_folder/custom_persisted_task With prefect-aws : from prefect import task from prefect_aws import S3Bucket from symph.prefect_utils.blocks import task_persistence_subfolder bucket: S3Bucket = S3Bucket(bucket_folder=\"base_folder/\") @task_persistence_subfolder(bucket) @task(persist_result=True) def persisted_task(data: dict) -> dict: '''A task whose return value is persisted in a subfolder of `bucket`. ''' ... print(persisted_task.result_storage.basepath) base_folder/persisted_task","title":"File Systems"},{"location":"filesystems/#filesystems","text":"Tools to extend Prefect file system behavior.","title":"Filesystems"},{"location":"filesystems/#create_child","text":"Create a child file system from a parent file system. If path is given, the child file system will have its basepath set to the resolved path. If suffix is given, it will be appended to the child block's _block_document_name . The FileSystem class must have a basepath or bucket_folder attribute and init parameter a _resolve_path method to resolve the path. path . It must also have a _resolve_path method to resolve the path. Parameters: fs ( FileSystem ) \u2013 the parent file system path ( str | Path ) \u2013 the path to the child file system suffix ( str ) \u2013 the suffix to append to the child block's Returns: FileSystem ( FileSystem ) \u2013 the child file system with a resolved basepath Examples: Basic usage: from prefect.filesystems import LocalFileSystem from prefecto.filesystems import create_child fs = LocalFileSystem(basepath=\"base_folder\") child = create_child(fs, path=\"child_folder\") print(child.basepath) base_folder/child_folder With prefect-aws : from prefect_aws import S3Bucket from prefecto.filesystems import create_child fs = S3Bucket(bucket_folder=\"base_folder\") child = create_child(fs, path=\"child_folder\") print(child.bucket_folder) base_folder/child_folder","title":"create_child"},{"location":"filesystems/#task_persistence_subfolder","text":"Decorator to set the task.result_storage attribute to a child file system of fs . This method does not alter the result_serializer or persist_result attributes. Parameters: fs ( FileSystem ) \u2013 the file system to create a child file system from path ( str ) \u2013 the path to the child file system. Defaults to None. suffix ( str ) \u2013 the suffix to append to the child block's _block_document_name . Defaults to None. Returns: Callable \u2013 the decorator to apply to a task Examples: Basic usage: from prefect import task from prefect.filesystems import LocalFileSystem from prefecto.filesystems import task_persistence_subfolder fs = LocalFileSystem(basepath=\"base_folder/\") @task_persistence_subfolder(fs) @task(persist_result=True) def persisted_task(): ... print(persisted_task.result_storage.basepath) base_folder/persisted_task Custom path: from prefect import task from prefect.filesystems import LocalFileSystem from prefecto.filesystems import task_persistence_subfolder fs = LocalFileSystem(basepath=\"base_folder/\") @task_persistence_subfolder(fs, path=\"custom_persisted_task\") @task(persist_result=True) def persisted_task(): ... print(persisted_task.result_storage.basepath) base_folder/custom_persisted_task With prefect-aws : from prefect import task from prefect_aws import S3Bucket from symph.prefect_utils.blocks import task_persistence_subfolder bucket: S3Bucket = S3Bucket(bucket_folder=\"base_folder/\") @task_persistence_subfolder(bucket) @task(persist_result=True) def persisted_task(data: dict) -> dict: '''A task whose return value is persisted in a subfolder of `bucket`. ''' ... print(persisted_task.result_storage.basepath) base_folder/persisted_task","title":"task_persistence_subfolder"},{"location":"license/","text":"License MIT License Copyright (c) 2023 Dominic Tarro Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright (c) 2023 Dominic Tarro Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"serializers/pandas/","text":"Pandas Serialization module for pandas.DataFrame . PandasSerializer Type: pandas Bases: ExtendedSerializer Serializer for pandas.DataFrame objects. Parameters: method ( str ) \u2013 The method to use for reading and writing. Must be a registered Method . Defaults to \"pandas.tsv\". read_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the read method. Overrides default arguments for the method. write_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the write method. Overrides default arguments for the method. Examples: Simple read and write. import pandas as pd from prefecto.serializers.pandas import PandasSerializer df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) blob = PandasSerializer().dumps(df) df2 = PandasSerializer().loads(blob) assert df2.equals(df) Using custom read and write kwargs. blob = PandasSerializer(write_kwargs={\"index\": True}).dumps(df) df2 = PandasSerializer(read_kwargs={\"index_col\": 0}).loads(blob) assert df2.equals(df) Methods Method classes for serializing pandas.DataFrame . Discriminator Default Read Default Write pandas.csv {\"index\": False} {\"index\": False} pandas.excel None None pandas.feather None None pandas.json None None pandas.jsonl None None pandas.parquet None None pandas.pickle None None pandas.tsv {\"sep\": \"\\t\", \"index\": False} {\"sep\": \"\\t\", \"index\": False}","title":"Pandas"},{"location":"serializers/pandas/#pandas","text":"Serialization module for pandas.DataFrame .","title":"Pandas"},{"location":"serializers/pandas/#pandasserializer","text":"Type: pandas Bases: ExtendedSerializer Serializer for pandas.DataFrame objects. Parameters: method ( str ) \u2013 The method to use for reading and writing. Must be a registered Method . Defaults to \"pandas.tsv\". read_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the read method. Overrides default arguments for the method. write_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the write method. Overrides default arguments for the method. Examples: Simple read and write. import pandas as pd from prefecto.serializers.pandas import PandasSerializer df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) blob = PandasSerializer().dumps(df) df2 = PandasSerializer().loads(blob) assert df2.equals(df) Using custom read and write kwargs. blob = PandasSerializer(write_kwargs={\"index\": True}).dumps(df) df2 = PandasSerializer(read_kwargs={\"index_col\": 0}).loads(blob) assert df2.equals(df)","title":"PandasSerializer"},{"location":"serializers/pandas/#methods","text":"Method classes for serializing pandas.DataFrame . Discriminator Default Read Default Write pandas.csv {\"index\": False} {\"index\": False} pandas.excel None None pandas.feather None None pandas.json None None pandas.jsonl None None pandas.parquet None None pandas.pickle None None pandas.tsv {\"sep\": \"\\t\", \"index\": False} {\"sep\": \"\\t\", \"index\": False}","title":"Methods"},{"location":"serializers/polars/","text":"Polars Serialization module for polars.DataFrame . PolarsSerializer Type: polars Bases: ExtendedSerializer Serializer for polars.DataFrame objects. Parameters: method ( str ) \u2013 The method to use for reading and writing. Must be a registered Method . Defaults to \"polars.parquet\". read_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the read method. Overrides default arguments for the method. write_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the write method. Overrides default arguments for the method. Examples: Simple read and write. import polars as pl from prefecto.serializers.polars import PolarsSerializer df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) blob = PolarsSerializer().dumps(df) print(blob) df2 = PolarsSerializer().loads(blob) assert df2.frame_equal(df) True Using a different method. blob = PolarsSerializer(method=\"polars.csv\").dumps(df) df2 = PolarsSerializer(method=\"polars.csv\").loads(blob) assert df2.frame_equal(df) True Using custom read and write kwargs. blob = PolarsSerializer(write_kwargs={\"use_pyarrow\": True}).dumps(df) df2 = PolarsSerializer(read_kwargs={\"use_pyarrow\": True}).loads(blob) assert df2.frame_equal(df) Methods Method classes for serializing polars.DataFrame . Discriminator Default Read Default Write polars.csv None None polars.excel None None polars.json None None polars.ndjson None None polars.parquet None None polars.tsv {\"separator\": \"\\t\"} {\"separator\": \"\\t\"}","title":"Polars"},{"location":"serializers/polars/#polars","text":"Serialization module for polars.DataFrame .","title":"Polars"},{"location":"serializers/polars/#polarsserializer","text":"Type: polars Bases: ExtendedSerializer Serializer for polars.DataFrame objects. Parameters: method ( str ) \u2013 The method to use for reading and writing. Must be a registered Method . Defaults to \"polars.parquet\". read_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the read method. Overrides default arguments for the method. write_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the write method. Overrides default arguments for the method. Examples: Simple read and write. import polars as pl from prefecto.serializers.polars import PolarsSerializer df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) blob = PolarsSerializer().dumps(df) print(blob) df2 = PolarsSerializer().loads(blob) assert df2.frame_equal(df) True Using a different method. blob = PolarsSerializer(method=\"polars.csv\").dumps(df) df2 = PolarsSerializer(method=\"polars.csv\").loads(blob) assert df2.frame_equal(df) True Using custom read and write kwargs. blob = PolarsSerializer(write_kwargs={\"use_pyarrow\": True}).dumps(df) df2 = PolarsSerializer(read_kwargs={\"use_pyarrow\": True}).loads(blob) assert df2.frame_equal(df)","title":"PolarsSerializer"},{"location":"serializers/polars/#methods","text":"Method classes for serializing polars.DataFrame . Discriminator Default Read Default Write polars.csv None None polars.excel None None polars.json None None polars.ndjson None None polars.parquet None None polars.tsv {\"separator\": \"\\t\"} {\"separator\": \"\\t\"}","title":"Methods"},{"location":"serializers/serialization/","text":"Serialization Library for serializing various data types. ExtendedSerializer Type: ext Bases: Serializer Extends the Serializer class to allow for custom serializers to be registered with their own methods for reading and writing. Good for complex types with standard read and write methods. Parameters: method ( str ) \u2013 The method to use for reading and writing. Must be a registered Method . read_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the read method. Overrides default arguments for the method. write_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the write method. Overrides default arguments for the method. Examples: from prefecto.serializers import ExtendedSerializer, Method, get_method def read(io: IO, encoding: str = 'utf-8') -> str: return io.read().decode(encoding) def write(value: str, io: IO, encoding: str = 'utf-8') -> None: io.write(value.encode(encoding)) class Utf8(Method): discriminator = \"utf8\" __read__ = read __write__ = write class Latin1(Method): discriminator = \"latin1\" __read__ = read __write__ = write default_read_kwargs = {\"encoding\": \"latin1\"} default_write_kwargs = {\"encoding\": \"latin1\"} ExtendedSerializer(\"utf8\").dumps(\"Hello, world!\") b'Hello, world!' get_method () Gets the Method to read and write objects with. dumps ( value ) Serialize the object with Method.write . Parameters: value ( Any ) \u2013 The object to serialize. Returns: bytes ( bytes ) \u2013 The serialized object. loads ( value ) Deserialize the object with Method.read . Parameters: value ( bytes ) \u2013 The serialized object. Returns: Any ( Any ) \u2013 The deserialized object. Method Base class for creating serialization methods. A method for reading and writing a type. To be subclassed for access via its discriminator. Parameters: discriminator ( str ) \u2013 The discriminator for the method. This must be globally unique. __read__ ( Callable ) \u2013 The function to read the object. __write__ ( Callable ) \u2013 The function to write the object. default_read_kwargs ( dict [ str , Any ] ) \u2013 Default keyword arguments for the read function. Must accept a BytesIO object as the first argument. default_write_kwargs ( dict [ str , Any ] ) \u2013 Default keyword arguments for the write function. Must accept the object to serialize as the first argument and a BytesIO object as the second argument. Examples: from io import IOBase as IO from prefecto.serializers import Method def read(io: IO, encoding: str = 'utf-8') -> str: return io.read().decode(encoding) def write(value: str, io: IO, encoding: str = 'utf-8') -> None: io.write(value.encode(encoding)) class Utf8(Method): discriminator = \"utf8\" __read__ = read __write__ = write class Latin1(Method): discriminator = \"latin1\" __read__ = read __write__ = write default_read_kwargs = {\"encoding\": \"latin1\"} default_write_kwargs = {\"encoding\": \"latin1\"} read ( * args , ** kwargs ) classmethod Reads the object. write ( * args , ** kwargs ) classmethod Writes the object. get_method Gets the Method by its discriminator. Parameters: discriminator ( str ) \u2013 The discriminator for the method. Raises: KeyError \u2013 If the discriminator is not registered. Returns: Method ( Method ) \u2013 The method.","title":"Serialization"},{"location":"serializers/serialization/#serialization","text":"Library for serializing various data types.","title":"Serialization"},{"location":"serializers/serialization/#extendedserializer","text":"Type: ext Bases: Serializer Extends the Serializer class to allow for custom serializers to be registered with their own methods for reading and writing. Good for complex types with standard read and write methods. Parameters: method ( str ) \u2013 The method to use for reading and writing. Must be a registered Method . read_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the read method. Overrides default arguments for the method. write_kwargs ( dict [ str , Any ] ) \u2013 Keyword arguments for the write method. Overrides default arguments for the method. Examples: from prefecto.serializers import ExtendedSerializer, Method, get_method def read(io: IO, encoding: str = 'utf-8') -> str: return io.read().decode(encoding) def write(value: str, io: IO, encoding: str = 'utf-8') -> None: io.write(value.encode(encoding)) class Utf8(Method): discriminator = \"utf8\" __read__ = read __write__ = write class Latin1(Method): discriminator = \"latin1\" __read__ = read __write__ = write default_read_kwargs = {\"encoding\": \"latin1\"} default_write_kwargs = {\"encoding\": \"latin1\"} ExtendedSerializer(\"utf8\").dumps(\"Hello, world!\") b'Hello, world!'","title":"ExtendedSerializer"},{"location":"serializers/serialization/#src.prefecto.serializers.core.ExtendedSerializer.get_method","text":"Gets the Method to read and write objects with.","title":"get_method()"},{"location":"serializers/serialization/#src.prefecto.serializers.core.ExtendedSerializer.dumps","text":"Serialize the object with Method.write . Parameters: value ( Any ) \u2013 The object to serialize. Returns: bytes ( bytes ) \u2013 The serialized object.","title":"dumps()"},{"location":"serializers/serialization/#src.prefecto.serializers.core.ExtendedSerializer.loads","text":"Deserialize the object with Method.read . Parameters: value ( bytes ) \u2013 The serialized object. Returns: Any ( Any ) \u2013 The deserialized object.","title":"loads()"},{"location":"serializers/serialization/#method","text":"Base class for creating serialization methods. A method for reading and writing a type. To be subclassed for access via its discriminator. Parameters: discriminator ( str ) \u2013 The discriminator for the method. This must be globally unique. __read__ ( Callable ) \u2013 The function to read the object. __write__ ( Callable ) \u2013 The function to write the object. default_read_kwargs ( dict [ str , Any ] ) \u2013 Default keyword arguments for the read function. Must accept a BytesIO object as the first argument. default_write_kwargs ( dict [ str , Any ] ) \u2013 Default keyword arguments for the write function. Must accept the object to serialize as the first argument and a BytesIO object as the second argument. Examples: from io import IOBase as IO from prefecto.serializers import Method def read(io: IO, encoding: str = 'utf-8') -> str: return io.read().decode(encoding) def write(value: str, io: IO, encoding: str = 'utf-8') -> None: io.write(value.encode(encoding)) class Utf8(Method): discriminator = \"utf8\" __read__ = read __write__ = write class Latin1(Method): discriminator = \"latin1\" __read__ = read __write__ = write default_read_kwargs = {\"encoding\": \"latin1\"} default_write_kwargs = {\"encoding\": \"latin1\"}","title":"Method"},{"location":"serializers/serialization/#src.prefecto.serializers.core.Method.read","text":"Reads the object.","title":"read()"},{"location":"serializers/serialization/#src.prefecto.serializers.core.Method.write","text":"Writes the object.","title":"write()"},{"location":"serializers/serialization/#get_method","text":"Gets the Method by its discriminator. Parameters: discriminator ( str ) \u2013 The discriminator for the method. Raises: KeyError \u2013 If the discriminator is not registered. Returns: Method ( Method ) \u2013 The method.","title":"get_method"},{"location":"testing/s3/","text":"S3 mock_bucket Creates a mock S3 bucket with moto . If given an export path, the mock bucket will export its contents during teardown. Parameters: bucket_name ( str ) \u2013 The name of the bucket. export_path ( str | Path | None ) \u2013 The path to export the bucket contents to. keys ( list [ str ] | None ) \u2013 The keys to export. If None, all keys will be exported. processes ( int ) \u2013 The number of threads to use for exporting. Defaults to 3. chunksize ( int ) \u2013 The chunksize to use for exporting. Defaults to 5. activate_moto ( bool ) \u2013 Whether to activate the moto mock environment. Defaults to True. Yields: Any \u2013 The mocked bucket. Examples: You can use the context manager to mock a bucket in your tests without exporting its contents: from prefecto.testing.s3 import mock_bucket with mock_bucket(\"my-bucket\") as my_bucket: # Do stuff with bucket ... Or you can create a fixture that exports the bucket's contents to a directory: Useful for auditing folder structures and file contents after local tests. from prefecto.testing.s3 import mock_bucket with mock_bucket(\"my-bucket\", export_path=\"path/to/export/dir\") as my_bucket: my_bucket.put_object(Key=\"test-key-1.txt\", Body=b\"test value 1\") my_bucket.put_object(Key=\"subfolder/test-key-2.txt\", Body=b\"test value 2\") path/ \u2514\u2500\u2500 to/ \u2514\u2500\u2500 export/ \u2514\u2500\u2500 dir/ \u2514\u2500\u2500 my-bucket/ |\u2500\u2500 test-key-1.txt \u2514\u2500\u2500 subfolder/ \u2514\u2500\u2500 test-key-2.txt You can deactivate moto's mock environment by setting activate_moto=False . This is useful for nesting mock buckets or mocking multiple buckets in the same context. from prefecto.testing.s3 import mock_bucket with mock_bucket(\"my-bucket\", export_path=\"path/to/export/dir\") as my_bucket: my_bucket.put_object(Key=\"test-key-1.txt\", Body=b\"test value 1\") with mock_bucket(\"my-bucket-2\", export_path=\"path/to/export/dir\", activate_moto=False) as my_bucket_2: my_bucket_2.put_object(Key=\"test-key-2.txt\", Body=b\"test value 2\") path/ \u2514\u2500\u2500 to/ \u2514\u2500\u2500 export/ \u2514\u2500\u2500 dir/ \u251c\u2500\u2500 my-bucket/ | \u2514\u2500\u2500 test-key-1.txt \u2514\u2500\u2500 my-bucket-2/ \u2514\u2500\u2500 test-key-2.txt","title":"S3"},{"location":"testing/s3/#s3","text":"","title":"S3"},{"location":"testing/s3/#mock_bucket","text":"Creates a mock S3 bucket with moto . If given an export path, the mock bucket will export its contents during teardown. Parameters: bucket_name ( str ) \u2013 The name of the bucket. export_path ( str | Path | None ) \u2013 The path to export the bucket contents to. keys ( list [ str ] | None ) \u2013 The keys to export. If None, all keys will be exported. processes ( int ) \u2013 The number of threads to use for exporting. Defaults to 3. chunksize ( int ) \u2013 The chunksize to use for exporting. Defaults to 5. activate_moto ( bool ) \u2013 Whether to activate the moto mock environment. Defaults to True. Yields: Any \u2013 The mocked bucket. Examples: You can use the context manager to mock a bucket in your tests without exporting its contents: from prefecto.testing.s3 import mock_bucket with mock_bucket(\"my-bucket\") as my_bucket: # Do stuff with bucket ... Or you can create a fixture that exports the bucket's contents to a directory: Useful for auditing folder structures and file contents after local tests. from prefecto.testing.s3 import mock_bucket with mock_bucket(\"my-bucket\", export_path=\"path/to/export/dir\") as my_bucket: my_bucket.put_object(Key=\"test-key-1.txt\", Body=b\"test value 1\") my_bucket.put_object(Key=\"subfolder/test-key-2.txt\", Body=b\"test value 2\") path/ \u2514\u2500\u2500 to/ \u2514\u2500\u2500 export/ \u2514\u2500\u2500 dir/ \u2514\u2500\u2500 my-bucket/ |\u2500\u2500 test-key-1.txt \u2514\u2500\u2500 subfolder/ \u2514\u2500\u2500 test-key-2.txt You can deactivate moto's mock environment by setting activate_moto=False . This is useful for nesting mock buckets or mocking multiple buckets in the same context. from prefecto.testing.s3 import mock_bucket with mock_bucket(\"my-bucket\", export_path=\"path/to/export/dir\") as my_bucket: my_bucket.put_object(Key=\"test-key-1.txt\", Body=b\"test value 1\") with mock_bucket(\"my-bucket-2\", export_path=\"path/to/export/dir\", activate_moto=False) as my_bucket_2: my_bucket_2.put_object(Key=\"test-key-2.txt\", Body=b\"test value 2\") path/ \u2514\u2500\u2500 to/ \u2514\u2500\u2500 export/ \u2514\u2500\u2500 dir/ \u251c\u2500\u2500 my-bucket/ | \u2514\u2500\u2500 test-key-1.txt \u2514\u2500\u2500 my-bucket-2/ \u2514\u2500\u2500 test-key-2.txt","title":"mock_bucket"}]}